# Master's Project â€“ Adversarial Attacks on Machine Learning Models

## Overview
This Master's project focuses on evaluating the security and robustness of
machine learning models against adversarial attacks.

The project demonstrates how attackers can manipulate input data to fool
machine learning models and explores mitigation techniques.

## Objectives
- Analyze vulnerabilities in ML models
- Implement FGSM and PGD adversarial attacks
- Measure performance degradation
- Study defensive strategies

## Tools & Technologies
- Python
- TensorFlow / PyTorch
- FGSM (Fast Gradient Sign Method)
- PGD (Projected Gradient Descent)
- Jupyter Notebook

## Project Documentation
ðŸ“„ **Project Report**:  
[Click here to view the Master's Project Report](Masters_Project_Report.pdf)

## Key Results
- FGSM attack reduced model accuracy significantly
- PGD attack showed stronger adversarial effectiveness
- Highlighted security risks in deployed ML systems

## Domain
- Cybersecurity
- Machine Learning Security
- Adversarial AI
